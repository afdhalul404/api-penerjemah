{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Layer, Add, GRU, \\\n",
    "                                    Activation, Softmax, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Tolaki                              Ina\n",
      "0        aria a mbotingu             dari dalam keranjang\n",
      "1               aa laika                      dalam rumah\n",
      "2            aa no monga             pinggangnya langsing\n",
      "3  monaa wata pe'a pe'aa  menyimpan batang yang berlubang\n",
      "4                aa enge                    lubang hidung\n",
      "<bound method DataFrame.__len__ of                                                  Tolaki  \\\n",
      "0                                       aria a mbotingu   \n",
      "1                                              aa laika   \n",
      "2                                           aa no monga   \n",
      "3                                 monaa wata pe'a pe'aa   \n",
      "4                                               aa enge   \n",
      "...                                                 ...   \n",
      "4511                   poworea o hule ariito pinokolako   \n",
      "4512  ie banggonahakono tai-tai rota ona suui-tudui ...   \n",
      "4513  ano'ene alei humnggai'ipalako tonggoitongano a...   \n",
      "4514  tolea laa lako mesambepo no laa nio molasu oti...   \n",
      "4515  lakorokaa mbendekambe'ale' i sarunggaro arombe...   \n",
      "\n",
      "                                                    Ina  \n",
      "0                                  dari dalam keranjang  \n",
      "1                                           dalam rumah  \n",
      "2                                  pinggangnya langsing  \n",
      "3                       menyimpan batang yang berlubang  \n",
      "4                                         lubang hidung  \n",
      "...                                                 ...  \n",
      "4511              telah dilaksanakan pembedahan jantung  \n",
      "4512  teman-temannya berjajar akan mencoba dalamnya ...  \n",
      "4513  dia pergi mengambil tempat sirih, lalu dibuka ...  \n",
      "4514  pembicara adat sementara pergi menghubungi ada...  \n",
      "4515  mereka terus bubar mengambil pakaian samaran m...  \n",
      "\n",
      "[4516 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('../data.xlsx')\n",
    "df = df.dropna()\n",
    "print(df.head(5))\n",
    "print(df.__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())  \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) \n",
    "    w = re.sub(r'[\" \"]+', \" \", w) \n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip() \n",
    "    if not w.startswith('<start>'):\n",
    "        w = '<start> ' + w\n",
    "    if not w.endswith('<end>'):\n",
    "        w = w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolaki    0\n",
      "Ina       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Ina'] = df['Ina'].apply(preprocess_sentence)\n",
    "df['Tolaki'] = df['Tolaki'].apply(preprocess_sentence)\n",
    "df.drop([2], axis=0, inplace=True)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "limiter = 50000\n",
    "df = df.iloc[:limiter, :]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# Buat tokenizer dari data Anda\n",
    "tokenizer_input = create_tokenizer(df['Ina'])\n",
    "tokenizer_output = create_tokenizer(df['Tolaki'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "length_input = max_length(df['Ina'])\n",
    "length_output = max_length(df['Tolaki'])\n",
    "\n",
    "\n",
    "print(length_output)\n",
    "print(length_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tolaki\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "\n",
    "# For Ina\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "lang_input = encode_text(tokenizer_input, df['Ina'], length_input)\n",
    "lang_output = encode_text(tokenizer_output, df['Tolaki'], length_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n",
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer_input))  # Harusnya <class 'tensorflow.keras.preprocessing.text.Tokenizer'>\n",
    "print(type(tokenizer_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ina_tolaki_tokenizer_input.json', 'w') as f:\n",
    "    json.dump(tokenizer_input.to_json(), f)\n",
    "\n",
    "with open('ina_tolaki_tokenizer_output.json', 'w') as f:\n",
    "    json.dump(tokenizer_output.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n",
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Muat tokenizer dari JSON\n",
    "with open('ina_tolaki_tokenizer_input.json', 'r') as f:\n",
    "    tokenizer_input_json = json.load(f)\n",
    "    tokenizer_input = tokenizer_from_json(tokenizer_input_json)\n",
    "\n",
    "with open('ina_tolaki_tokenizer_output.json', 'r') as f:\n",
    "    tokenizer_output_json = json.load(f)\n",
    "    tokenizer_output = tokenizer_from_json(tokenizer_output_json)\n",
    "\n",
    "# Verifikasi tipe\n",
    "print(type(tokenizer_input)) \n",
    "print(type(tokenizer_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inp_embed_size = 128\n",
    "inp_lstm_cells = 256\n",
    "tar_embed_size = 128\n",
    "tar_lstm_cells = 256\n",
    "attention_units = 256\n",
    "hidden_state = [tf.zeros([batch_size, inp_lstm_cells])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, inp_vocab_size, inp_embed_size, inp_lstm_cells, batch_size, inp_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.inp_embed_size = inp_embed_size \n",
    "        self.inp_vocab_size = inp_vocab_size \n",
    "        self.inp_lstm_cells = inp_lstm_cells \n",
    "        self.batch_size = batch_size         \n",
    "        self.inp_len = inp_len               \n",
    "        self.enc_embedding = Embedding(self.inp_vocab_size, self.inp_embed_size, trainable=True)\n",
    "        self.lstm = LSTM(self.inp_lstm_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, inp_sequence, hidden_sequence):\n",
    "        emb_output = self.enc_embedding(inp_sequence)\n",
    "        inp_lstm_output, state_h, state_c = self.lstm(emb_output, initial_state = hidden_sequence)\n",
    "        return inp_lstm_output, state_h\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        return [tf.zeros([self.batch_size, self.inp_lstm_cells]), tf.zeros([self.batch_size, self.inp_lstm_cells])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = Encoder(inp_vocab_size=vocab_size_input, inp_embed_size=inp_embed_size, inp_lstm_cells=inp_lstm_cells,\n",
    "                    batch_size=batch_size, inp_len=length_input)\n",
    "initialized_hidden_states = enc_model.initialize_hidden_states()\n",
    "\n",
    "enc_out, enc_state = enc_model(lang_input[:batch_size], initialized_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(attention_units)\n",
    "        self.W2 = tf.keras.layers.Dense(attention_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden, output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(hidden_with_time_axis) + self.W2(output)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, tar_embed_size, tar_vocab_size, tar_lstm_cells, attention_units,\n",
    "                 batch_size=batch_size, tar_len=length_output):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tar_embed_size = tar_embed_size \n",
    "        self.tar_vocab_size = tar_vocab_size \n",
    "        self.tar_lstm_cells = tar_lstm_cells \n",
    "        self.batch_size = batch_size         \n",
    "        self.tar_len = tar_len              \n",
    "        self.attention_units = attention_units  \n",
    "        self.dec_embedding = Embedding(self.tar_vocab_size, self.tar_embed_size, trainable=True)\n",
    "        self.lstm = LSTM(self.tar_lstm_cells, return_sequences=True, return_state=True)\n",
    "        self.attention = Attention(self.attention_units)\n",
    "        self.final_layer = Dense(self.tar_vocab_size)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        emb_output = self.dec_embedding(x)\n",
    "        x_context = tf.concat([tf.expand_dims(context_vector, axis=1), emb_output], axis=-1)\n",
    "        tar_lstm_output, tar_state_h, tar_state_c = self.lstm(x_context)\n",
    "        tar_lstm_output_reshaped = tf.reshape(tar_lstm_output, shape=(-1, tar_lstm_output.shape[2]))\n",
    "        word_prob = self.final_layer(tar_lstm_output_reshaped)\n",
    "        return word_prob, tar_state_h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_model = Decoder(tar_embed_size=tar_embed_size, tar_vocab_size=vocab_size_output, tar_lstm_cells=tar_lstm_cells,\n",
    "                    attention_units=attention_units)\n",
    "dec_out, dec_state, atn_w = dec_model(tf.random.uniform((batch_size, 1)), enc_state, enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(lang_inp, lang_out):\n",
    "    n_elem = lang_inp.shape[0]\n",
    "    indices = np.random.choice(n_elem, size=n_elem, replace=False)\n",
    "    return lang_inp[indices], lang_out[indices]\n",
    "\n",
    "def generator(batch_number, lang_input, lang_output):\n",
    "    if len(lang_input) <= batch_number*batch_size+batch_size:\n",
    "        return (lang_input[batch_number*batch_size:],\n",
    "            lang_output[batch_number*batch_size:])\n",
    "    return (lang_input[batch_number*batch_size: batch_number*batch_size+batch_size],\n",
    "            lang_output[batch_number*batch_size: batch_number*batch_size+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = enc_model(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([tokenizer_output.word_index['start']] * batch_size, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          predictions, dec_hidden, _ = dec_model(dec_input, dec_hidden, enc_output)\n",
    "          loss += loss_function(targ[:, t], predictions)\n",
    "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = enc_model.trainable_variables + dec_model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.6761\n",
      "Epoch 1 Loss 1.4292\n",
      "Time taken for 1 epoch 95.58161401748657 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5810\n",
      "Epoch 2 Loss 1.2877\n",
      "Time taken for 1 epoch 63.9338903427124 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3390\n",
      "Epoch 3 Loss 1.2461\n",
      "Time taken for 1 epoch 64.5710723400116 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2586\n",
      "Epoch 4 Loss 1.2111\n",
      "Time taken for 1 epoch 72.28937792778015 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3103\n",
      "Epoch 5 Loss 1.1716\n",
      "Time taken for 1 epoch 70.24094796180725 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9230\n",
      "Epoch 6 Loss 1.1250\n",
      "Time taken for 1 epoch 74.649982213974 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9480\n",
      "Epoch 7 Loss 1.0737\n",
      "Time taken for 1 epoch 72.11609315872192 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0760\n",
      "Epoch 8 Loss 1.0288\n",
      "Time taken for 1 epoch 67.8747787475586 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.8710\n",
      "Epoch 9 Loss 0.9799\n",
      "Time taken for 1 epoch 64.5176432132721 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.0823\n",
      "Epoch 10 Loss 0.9351\n",
      "Time taken for 1 epoch 63.676095485687256 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.9044\n",
      "Epoch 11 Loss 0.8909\n",
      "Time taken for 1 epoch 63.65637731552124 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0254\n",
      "Epoch 12 Loss 0.8483\n",
      "Time taken for 1 epoch 63.97051405906677 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.6134\n",
      "Epoch 13 Loss 0.8061\n",
      "Time taken for 1 epoch 63.663878440856934 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.7276\n",
      "Epoch 14 Loss 0.7641\n",
      "Time taken for 1 epoch 63.674930572509766 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.8110\n",
      "Epoch 15 Loss 0.7213\n",
      "Time taken for 1 epoch 63.45554065704346 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.6595\n",
      "Epoch 16 Loss 0.6814\n",
      "Time taken for 1 epoch 63.32814645767212 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.6283\n",
      "Epoch 17 Loss 0.6421\n",
      "Time taken for 1 epoch 63.36021161079407 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.6464\n",
      "Epoch 18 Loss 0.6052\n",
      "Time taken for 1 epoch 71.63717412948608 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5825\n",
      "Epoch 19 Loss 0.5694\n",
      "Time taken for 1 epoch 73.32885265350342 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5981\n",
      "Epoch 20 Loss 0.5358\n",
      "Time taken for 1 epoch 67.2141785621643 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4617\n",
      "Epoch 21 Loss 0.5021\n",
      "Time taken for 1 epoch 66.92467284202576 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.4440\n",
      "Epoch 22 Loss 0.4691\n",
      "Time taken for 1 epoch 63.20820164680481 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.4359\n",
      "Epoch 23 Loss 0.4413\n",
      "Time taken for 1 epoch 62.389246225357056 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4316\n",
      "Epoch 24 Loss 0.4123\n",
      "Time taken for 1 epoch 62.326958417892456 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.3637\n",
      "Epoch 25 Loss 0.3879\n",
      "Time taken for 1 epoch 63.41008949279785 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.3860\n",
      "Epoch 26 Loss 0.3630\n",
      "Time taken for 1 epoch 65.25368928909302 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.3057\n",
      "Epoch 27 Loss 0.3400\n",
      "Time taken for 1 epoch 63.55776238441467 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.2758\n",
      "Epoch 28 Loss 0.3195\n",
      "Time taken for 1 epoch 62.6078143119812 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.2774\n",
      "Epoch 29 Loss 0.3005\n",
      "Time taken for 1 epoch 62.64216589927673 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.2547\n",
      "Epoch 30 Loss 0.2817\n",
      "Time taken for 1 epoch 62.56940722465515 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.1976\n",
      "Epoch 31 Loss 0.2649\n",
      "Time taken for 1 epoch 62.133047342300415 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2072\n",
      "Epoch 32 Loss 0.2503\n",
      "Time taken for 1 epoch 61.51697039604187 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.1838\n",
      "Epoch 33 Loss 0.2322\n",
      "Time taken for 1 epoch 61.75627636909485 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.1898\n",
      "Epoch 34 Loss 0.2177\n",
      "Time taken for 1 epoch 61.56348657608032 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.1770\n",
      "Epoch 35 Loss 0.2033\n",
      "Time taken for 1 epoch 61.83074188232422 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.1877\n",
      "Epoch 36 Loss 0.1909\n",
      "Time taken for 1 epoch 61.4948627948761 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.1569\n",
      "Epoch 37 Loss 0.1797\n",
      "Time taken for 1 epoch 61.461538553237915 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1564\n",
      "Epoch 38 Loss 0.1668\n",
      "Time taken for 1 epoch 61.65151047706604 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1329\n",
      "Epoch 39 Loss 0.1561\n",
      "Time taken for 1 epoch 61.66395926475525 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1424\n",
      "Epoch 40 Loss 0.1461\n",
      "Time taken for 1 epoch 61.43855571746826 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.1282\n",
      "Epoch 41 Loss 0.1372\n",
      "Time taken for 1 epoch 61.574713706970215 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1174\n",
      "Epoch 42 Loss 0.1259\n",
      "Time taken for 1 epoch 61.813775062561035 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0839\n",
      "Epoch 43 Loss 0.1157\n",
      "Time taken for 1 epoch 61.78384590148926 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1213\n",
      "Epoch 44 Loss 0.1079\n",
      "Time taken for 1 epoch 61.57593369483948 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0813\n",
      "Epoch 45 Loss 0.1000\n",
      "Time taken for 1 epoch 61.65891432762146 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0796\n",
      "Epoch 46 Loss 0.0932\n",
      "Time taken for 1 epoch 61.488120794296265 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0756\n",
      "Epoch 47 Loss 0.0883\n",
      "Time taken for 1 epoch 61.75858759880066 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0798\n",
      "Epoch 48 Loss 0.0837\n",
      "Time taken for 1 epoch 61.61078929901123 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0742\n",
      "Epoch 49 Loss 0.0765\n",
      "Time taken for 1 epoch 61.75999617576599 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0600\n",
      "Epoch 50 Loss 0.0696\n",
      "Time taken for 1 epoch 61.60437822341919 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0560\n",
      "Epoch 51 Loss 0.0651\n",
      "Time taken for 1 epoch 567.3242268562317 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0495\n",
      "Epoch 52 Loss 0.0598\n",
      "Time taken for 1 epoch 67.61685705184937 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0469\n",
      "Epoch 53 Loss 0.0568\n",
      "Time taken for 1 epoch 67.02216267585754 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0452\n",
      "Epoch 54 Loss 0.0545\n",
      "Time taken for 1 epoch 69.70890545845032 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0601\n",
      "Epoch 55 Loss 0.0566\n",
      "Time taken for 1 epoch 84.93619537353516 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0461\n",
      "Epoch 56 Loss 0.0529\n",
      "Time taken for 1 epoch 74.92279720306396 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0325\n",
      "Epoch 57 Loss 0.0483\n",
      "Time taken for 1 epoch 84.7755389213562 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0363\n",
      "Epoch 58 Loss 0.0440\n",
      "Time taken for 1 epoch 85.93030047416687 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0396\n",
      "Epoch 59 Loss 0.0373\n",
      "Time taken for 1 epoch 78.08670139312744 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0316\n",
      "Epoch 60 Loss 0.0339\n",
      "Time taken for 1 epoch 76.59633803367615 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0351\n",
      "Epoch 61 Loss 0.0313\n",
      "Time taken for 1 epoch 75.5412802696228 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0211\n",
      "Epoch 62 Loss 0.0291\n",
      "Time taken for 1 epoch 74.22421336174011 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0248\n",
      "Epoch 63 Loss 0.0280\n",
      "Time taken for 1 epoch 73.081063747406 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0309\n",
      "Epoch 64 Loss 0.0266\n",
      "Time taken for 1 epoch 73.32335686683655 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0206\n",
      "Epoch 65 Loss 0.0260\n",
      "Time taken for 1 epoch 73.35307192802429 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0185\n",
      "Epoch 66 Loss 0.0253\n",
      "Time taken for 1 epoch 73.23746085166931 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0291\n",
      "Epoch 67 Loss 0.0242\n",
      "Time taken for 1 epoch 73.52464818954468 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0185\n",
      "Epoch 68 Loss 0.0235\n",
      "Time taken for 1 epoch 73.32268285751343 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0225\n",
      "Epoch 69 Loss 0.0447\n",
      "Time taken for 1 epoch 79.0668306350708 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0631\n",
      "Epoch 70 Loss 0.0706\n",
      "Time taken for 1 epoch 73.7270736694336 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0464\n",
      "Epoch 71 Loss 0.0458\n",
      "Time taken for 1 epoch 73.09197950363159 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0262\n",
      "Epoch 72 Loss 0.0321\n",
      "Time taken for 1 epoch 73.09071850776672 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0159\n",
      "Epoch 73 Loss 0.0230\n",
      "Time taken for 1 epoch 73.80566668510437 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0153\n",
      "Epoch 74 Loss 0.0192\n",
      "Time taken for 1 epoch 74.16733837127686 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0140\n",
      "Epoch 75 Loss 0.0177\n",
      "Time taken for 1 epoch 73.44662690162659 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0161\n",
      "Epoch 76 Loss 0.0173\n",
      "Time taken for 1 epoch 73.6865668296814 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0159\n",
      "Epoch 77 Loss 0.0167\n",
      "Time taken for 1 epoch 73.67481327056885 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0129\n",
      "Epoch 78 Loss 0.0162\n",
      "Time taken for 1 epoch 72.82079601287842 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0142\n",
      "Epoch 79 Loss 0.0157\n",
      "Time taken for 1 epoch 70.61938309669495 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0100\n",
      "Epoch 80 Loss 0.0155\n",
      "Time taken for 1 epoch 70.22143864631653 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0119\n",
      "Epoch 81 Loss 0.0153\n",
      "Time taken for 1 epoch 71.6896619796753 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0068\n",
      "Epoch 82 Loss 0.0151\n",
      "Time taken for 1 epoch 70.00616145133972 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0116\n",
      "Epoch 83 Loss 0.0149\n",
      "Time taken for 1 epoch 68.75709223747253 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0111\n",
      "Epoch 84 Loss 0.0149\n",
      "Time taken for 1 epoch 68.58740019798279 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0104\n",
      "Epoch 85 Loss 0.0149\n",
      "Time taken for 1 epoch 69.19330644607544 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0094\n",
      "Epoch 86 Loss 0.0149\n",
      "Time taken for 1 epoch 69.43336176872253 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0137\n",
      "Epoch 87 Loss 0.0152\n",
      "Time taken for 1 epoch 68.68109107017517 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0094\n",
      "Epoch 88 Loss 0.0150\n",
      "Time taken for 1 epoch 68.89445853233337 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0168\n",
      "Epoch 89 Loss 0.0152\n",
      "Time taken for 1 epoch 69.03880882263184 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0110\n",
      "Epoch 90 Loss 0.0175\n",
      "Time taken for 1 epoch 68.65997648239136 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0144\n",
      "Epoch 91 Loss 0.0389\n",
      "Time taken for 1 epoch 63.85725212097168 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0725\n",
      "Epoch 92 Loss 0.0577\n",
      "Time taken for 1 epoch 63.75397706031799 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0298\n",
      "Epoch 93 Loss 0.0336\n",
      "Time taken for 1 epoch 63.460296869277954 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0160\n",
      "Epoch 94 Loss 0.0201\n",
      "Time taken for 1 epoch 63.51411700248718 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0141\n",
      "Epoch 95 Loss 0.0157\n",
      "Time taken for 1 epoch 63.058430671691895 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0226\n",
      "Epoch 96 Loss 0.0147\n",
      "Time taken for 1 epoch 62.999653577804565 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0110\n",
      "Epoch 97 Loss 0.0136\n",
      "Time taken for 1 epoch 63.506728649139404 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0055\n",
      "Epoch 98 Loss 0.0131\n",
      "Time taken for 1 epoch 63.00151610374451 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0069\n",
      "Epoch 99 Loss 0.0129\n",
      "Time taken for 1 epoch 63.38011980056763 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0181\n",
      "Epoch 100 Loss 0.0126\n",
      "Time taken for 1 epoch 63.116661071777344 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0112\n",
      "Epoch 101 Loss 0.0126\n",
      "Time taken for 1 epoch 63.517913818359375 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0087\n",
      "Epoch 102 Loss 0.0125\n",
      "Time taken for 1 epoch 63.87772727012634 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0111\n",
      "Epoch 103 Loss 0.0124\n",
      "Time taken for 1 epoch 63.52608323097229 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0029\n",
      "Epoch 104 Loss 0.0126\n",
      "Time taken for 1 epoch 63.18720054626465 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0087\n",
      "Epoch 105 Loss 0.0126\n",
      "Time taken for 1 epoch 63.59882140159607 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0158\n",
      "Epoch 106 Loss 0.0124\n",
      "Time taken for 1 epoch 63.86776638031006 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0064\n",
      "Epoch 107 Loss 0.0125\n",
      "Time taken for 1 epoch 63.014081954956055 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0128\n",
      "Epoch 108 Loss 0.0129\n",
      "Time taken for 1 epoch 63.10636281967163 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0080\n",
      "Epoch 109 Loss 0.0132\n",
      "Time taken for 1 epoch 3340.645450592041 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0046\n",
      "Epoch 110 Loss 0.0131\n",
      "Time taken for 1 epoch 65.89794588088989 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0047\n",
      "Epoch 111 Loss 0.0128\n",
      "Time taken for 1 epoch 62.89160656929016 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0068\n",
      "Epoch 112 Loss 0.0132\n",
      "Time taken for 1 epoch 63.19927954673767 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0085\n",
      "Epoch 113 Loss 0.0136\n",
      "Time taken for 1 epoch 67.13761234283447 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0041\n",
      "Epoch 114 Loss 0.0146\n",
      "Time taken for 1 epoch 64.63292479515076 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0151\n",
      "Epoch 115 Loss 0.0413\n",
      "Time taken for 1 epoch 65.0326247215271 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0456\n",
      "Epoch 116 Loss 0.0441\n",
      "Time taken for 1 epoch 65.60874629020691 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0238\n",
      "Epoch 117 Loss 0.0240\n",
      "Time taken for 1 epoch 64.19115138053894 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0149\n",
      "Epoch 118 Loss 0.0159\n",
      "Time taken for 1 epoch 64.57511496543884 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0079\n",
      "Epoch 119 Loss 0.0135\n",
      "Time taken for 1 epoch 64.50798630714417 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0089\n",
      "Epoch 120 Loss 0.0125\n",
      "Time taken for 1 epoch 68.57217025756836 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0087\n",
      "Epoch 121 Loss 0.0120\n",
      "Time taken for 1 epoch 64.53937363624573 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0049\n",
      "Epoch 122 Loss 0.0118\n",
      "Time taken for 1 epoch 64.00238847732544 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0210\n",
      "Epoch 123 Loss 0.0117\n",
      "Time taken for 1 epoch 63.70183801651001 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0059\n",
      "Epoch 124 Loss 0.0115\n",
      "Time taken for 1 epoch 64.07133436203003 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0061\n",
      "Epoch 125 Loss 0.0115\n",
      "Time taken for 1 epoch 63.928908586502075 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0104\n",
      "Epoch 126 Loss 0.0114\n",
      "Time taken for 1 epoch 63.753478050231934 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0145\n",
      "Epoch 127 Loss 0.0114\n",
      "Time taken for 1 epoch 63.522411823272705 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0112\n",
      "Epoch 128 Loss 0.0114\n",
      "Time taken for 1 epoch 63.50871968269348 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0050\n",
      "Epoch 129 Loss 0.0115\n",
      "Time taken for 1 epoch 64.11295223236084 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0101\n",
      "Epoch 130 Loss 0.0116\n",
      "Time taken for 1 epoch 63.502633571624756 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0080\n",
      "Epoch 131 Loss 0.0117\n",
      "Time taken for 1 epoch 63.70768976211548 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0083\n",
      "Epoch 132 Loss 0.0118\n",
      "Time taken for 1 epoch 63.524574756622314 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0048\n",
      "Epoch 133 Loss 0.0127\n",
      "Time taken for 1 epoch 63.74239444732666 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0033\n",
      "Epoch 134 Loss 0.0145\n",
      "Time taken for 1 epoch 63.24004793167114 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0091\n",
      "Epoch 135 Loss 0.0271\n",
      "Time taken for 1 epoch 66.08961462974548 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0216\n",
      "Epoch 136 Loss 0.0316\n",
      "Time taken for 1 epoch 64.10953903198242 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0129\n",
      "Epoch 137 Loss 0.0203\n",
      "Time taken for 1 epoch 64.69019985198975 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0166\n",
      "Epoch 138 Loss 0.0161\n",
      "Time taken for 1 epoch 64.28377294540405 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0106\n",
      "Epoch 139 Loss 0.0138\n",
      "Time taken for 1 epoch 63.71662211418152 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0075\n",
      "Epoch 140 Loss 0.0128\n",
      "Time taken for 1 epoch 64.02307057380676 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0121\n",
      "Epoch 141 Loss 0.0122\n",
      "Time taken for 1 epoch 63.95771312713623 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0035\n",
      "Epoch 142 Loss 0.0116\n",
      "Time taken for 1 epoch 63.85832738876343 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0072\n",
      "Epoch 143 Loss 0.0112\n",
      "Time taken for 1 epoch 64.07497310638428 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0083\n",
      "Epoch 144 Loss 0.0113\n",
      "Time taken for 1 epoch 63.77951121330261 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0072\n",
      "Epoch 145 Loss 0.0113\n",
      "Time taken for 1 epoch 63.73819446563721 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0028\n",
      "Epoch 146 Loss 0.0112\n",
      "Time taken for 1 epoch 63.863842248916626 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0115\n",
      "Epoch 147 Loss 0.0113\n",
      "Time taken for 1 epoch 64.73807644844055 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0092\n",
      "Epoch 148 Loss 0.0113\n",
      "Time taken for 1 epoch 65.9024920463562 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0073\n",
      "Epoch 149 Loss 0.0113\n",
      "Time taken for 1 epoch 64.00783801078796 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0149\n",
      "Epoch 150 Loss 0.0113\n",
      "Time taken for 1 epoch 64.9567940235138 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "number_of_batches = lang_input.shape[0]//batch_size\n",
    "\n",
    "lang_input_split = lang_input[:number_of_batches*batch_size]\n",
    "lang_output_split = lang_output[:number_of_batches*batch_size]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = enc_model.initialize_hidden_states()\n",
    "    total_loss = 0\n",
    "    lang_inp, lang_out = shuffler(lang_input_split, lang_output_split)\n",
    "    for batch_number in range(number_of_batches):\n",
    "        inp, targ = generator(batch_number, lang_inp, lang_out)\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch_number % 200 == 0:\n",
    "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch_number,\n",
    "                                                       batch_loss.numpy()))\n",
    "    loss_history.append(total_loss / number_of_batches)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / number_of_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.save_weights('ina_tolaki_encoder.weights.h5')\n",
    "dec_model.save_weights('ina_tolaki_decoder.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = 'start ' + w + ' end'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocessing_sentence(sentence)\n",
    "    inputs = [tokenizer_input.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                             maxlen=length_input,\n",
    "                                                             padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros([1, inp_lstm_cells]), tf.zeros([1, inp_lstm_cells])]\n",
    "    enc_out, enc_hidden = enc_model(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer_output.word_index['start']], 0)\n",
    "\n",
    "    for t in range(length_output):\n",
    "        predictions, dec_hidden, attention_weights = dec_model(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        if tokenizer_output.index_word[predicted_id] == 'end':\n",
    "            return result, sentence\n",
    "\n",
    "        result += tokenizer_output.index_word[predicted_id] + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    inp_vocab_size = len(tokenizer_input.word_index) + 1\n",
    "    tar_vocab_size = len(tokenizer_output.word_index) + 1\n",
    "\n",
    "    # Buat instance model\n",
    "    enc_model = Encoder(inp_vocab_size=inp_vocab_size, inp_embed_size=inp_embed_size, \n",
    "                        inp_lstm_cells=inp_lstm_cells, batch_size=batch_size, inp_len=length_input)\n",
    "    dec_model = Decoder(tar_embed_size=tar_embed_size, tar_vocab_size=tar_vocab_size, \n",
    "                        tar_lstm_cells=tar_lstm_cells, attention_units=attention_units, \n",
    "                        batch_size=batch_size, tar_len=length_output)\n",
    "\n",
    "    # Panggil model untuk inisialisasi variabel\n",
    "    dummy_input_enc = tf.zeros((batch_size, length_input), dtype=tf.int32)\n",
    "    hidden_states_enc = enc_model.initialize_hidden_states()\n",
    "    enc_model(dummy_input_enc, hidden_states_enc)\n",
    "\n",
    "    dummy_input_dec = tf.zeros((batch_size, 1), dtype=tf.int32)\n",
    "    hidden_state_enc = hidden_states_enc[0]\n",
    "    dummy_enc_output = tf.zeros((batch_size, length_input, inp_lstm_cells))\n",
    "    dec_model(dummy_input_dec, hidden_state_enc, dummy_enc_output)\n",
    "\n",
    "    # Muat bobot\n",
    "    try:\n",
    "        enc_model.load_weights('ina_tolaki_encoder.weights.h5')\n",
    "        dec_model.load_weights('ina_tolaki_decoder.weights.h5')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Verifikasi struktur model\n",
    "    print(\"Encoder model summary:\")\n",
    "    enc_model.summary()\n",
    "    print(\"Decoder model summary:\")\n",
    "    dec_model.summary()\n",
    "\n",
    "    return enc_model, dec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat tokenizer dari JSON\n",
    "with open('ina_tolaki_tokenizer_input.json', 'r') as f:\n",
    "    tokenizer_input_json = json.load(f)\n",
    "    tokenizer_input = tokenizer_from_json(tokenizer_input_json)\n",
    "\n",
    "with open('ina_tolaki_tokenizer_output.json', 'r') as f:\n",
    "    tokenizer_output_json = json.load(f)\n",
    "    tokenizer_output = tokenizer_from_json(tokenizer_output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">376,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>))       │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m376,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m,   │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m))       │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770,944</span> (2.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770,944\u001b[0m (2.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770,944</span> (2.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770,944\u001b[0m (2.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">558,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,841</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4362</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,121,034</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m558,336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m,    │       \u001b[38;5;34m656,384\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)         │ ?                      │       \u001b[38;5;34m131,841\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m4362\u001b[0m)             │     \u001b[38;5;34m1,121,034\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,467,595</span> (9.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,467,595\u001b[0m (9.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,467,595</span> (9.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,467,595\u001b[0m (9.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Memuat model\n",
    "enc_model, dec_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat Bahasa Indonesia: start dari dalam keranjang end\n",
      "Hasil Terjemahan: aria a mbotingu \n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan untuk menerjemahkan kalimat\n",
    "translated_sentence, original_sentence = evaluate(\"dari dalam keranjang\")\n",
    "print(f\"Kalimat Bahasa Indonesia: {original_sentence}\")\n",
    "print(f\"Hasil Terjemahan: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
