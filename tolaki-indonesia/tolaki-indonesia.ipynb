{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Layer, Add, GRU, \\\n",
    "                                    Activation, Softmax, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Tolaki                              Ina\n",
      "0        aria a mbotingu             dari dalam keranjang\n",
      "1               aa laika                      dalam rumah\n",
      "2            aa no monga             pinggangnya langsing\n",
      "3  monaa wata pe'a pe'aa  menyimpan batang yang berlubang\n",
      "4                aa enge                    lubang hidung\n",
      "<bound method DataFrame.__len__ of                                                  Tolaki  \\\n",
      "0                                       aria a mbotingu   \n",
      "1                                              aa laika   \n",
      "2                                           aa no monga   \n",
      "3                                 monaa wata pe'a pe'aa   \n",
      "4                                               aa enge   \n",
      "...                                                 ...   \n",
      "4511                   poworea o hule ariito pinokolako   \n",
      "4512  ie banggonahakono tai-tai rota ona suui-tudui ...   \n",
      "4513  ano'ene alei humnggai'ipalako tonggoitongano a...   \n",
      "4514  tolea laa lako mesambepo no laa nio molasu oti...   \n",
      "4515  lakorokaa mbendekambe'ale' i sarunggaro arombe...   \n",
      "\n",
      "                                                    Ina  \n",
      "0                                  dari dalam keranjang  \n",
      "1                                           dalam rumah  \n",
      "2                                  pinggangnya langsing  \n",
      "3                       menyimpan batang yang berlubang  \n",
      "4                                         lubang hidung  \n",
      "...                                                 ...  \n",
      "4511              telah dilaksanakan pembedahan jantung  \n",
      "4512  teman-temannya berjajar akan mencoba dalamnya ...  \n",
      "4513  dia pergi mengambil tempat sirih, lalu dibuka ...  \n",
      "4514  pembicara adat sementara pergi menghubungi ada...  \n",
      "4515  mereka terus bubar mengambil pakaian samaran m...  \n",
      "\n",
      "[4516 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('../data.xlsx')\n",
    "df = df.dropna()\n",
    "print(df.head(5))\n",
    "print(df.__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())  \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) \n",
    "    w = re.sub(r'[\" \"]+', \" \", w) \n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip() \n",
    "    if not w.startswith('<start>'):\n",
    "        w = '<start> ' + w\n",
    "    if not w.endswith('<end>'):\n",
    "        w = w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolaki    0\n",
      "Ina       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Ina'] = df['Ina'].apply(preprocess_sentence)\n",
    "df['Tolaki'] = df['Tolaki'].apply(preprocess_sentence)\n",
    "df.drop([2], axis=0, inplace=True)\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "limiter = 50000\n",
    "df = df.iloc[:limiter, :]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# Buat tokenizer dari data Anda\n",
    "tokenizer_input = create_tokenizer(df['Tolaki'])\n",
    "tokenizer_output = create_tokenizer(df['Ina'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "length_input = max_length(df['Tolaki'])\n",
    "length_output = max_length(df['Ina'])\n",
    "\n",
    "\n",
    "print(length_output)\n",
    "print(length_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tolaki\n",
    "vocab_size_input = len(tokenizer_input.word_index) + 1\n",
    "\n",
    "# For Ina\n",
    "vocab_size_output = len(tokenizer_output.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "lang_input = encode_text(tokenizer_input, df['Tolaki'], length_input)\n",
    "lang_output = encode_text(tokenizer_output, df['Ina'], length_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n",
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenizer_input))  # Harusnya <class 'tensorflow.keras.preprocessing.text.Tokenizer'>\n",
    "print(type(tokenizer_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan tokenizer ke JSON\n",
    "with open('tolaki_ina_tokenizer_input.json', 'w') as f:\n",
    "    json.dump(tokenizer_input.to_json(), f)\n",
    "\n",
    "with open('tolaki_ina_tokenizer_output.json', 'w') as f:\n",
    "    json.dump(tokenizer_output.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n",
      "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Muat tokenizer dari JSON\n",
    "with open('tolaki_ina_tokenizer_input.json', 'r') as f:\n",
    "    tokenizer_input_json = json.load(f)\n",
    "    tokenizer_input = tokenizer_from_json(tokenizer_input_json)\n",
    "\n",
    "with open('tolaki_ina_tokenizer_output.json', 'r') as f:\n",
    "    tokenizer_output_json = json.load(f)\n",
    "    tokenizer_output = tokenizer_from_json(tokenizer_output_json)\n",
    "\n",
    "# Verifikasi tipe\n",
    "print(type(tokenizer_input)) \n",
    "print(type(tokenizer_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inp_embed_size = 128\n",
    "inp_lstm_cells = 256\n",
    "tar_embed_size = 128\n",
    "tar_lstm_cells = 256\n",
    "attention_units = 256\n",
    "hidden_state = [tf.zeros([batch_size, inp_lstm_cells])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, inp_vocab_size, inp_embed_size, inp_lstm_cells, batch_size, inp_len):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.inp_embed_size = inp_embed_size \n",
    "        self.inp_vocab_size = inp_vocab_size \n",
    "        self.inp_lstm_cells = inp_lstm_cells \n",
    "        self.batch_size = batch_size         \n",
    "        self.inp_len = inp_len               \n",
    "        self.enc_embedding = Embedding(self.inp_vocab_size, self.inp_embed_size, trainable=True)\n",
    "        self.lstm = LSTM(self.inp_lstm_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, inp_sequence, hidden_sequence):\n",
    "        emb_output = self.enc_embedding(inp_sequence)\n",
    "        inp_lstm_output, state_h, state_c = self.lstm(emb_output, initial_state = hidden_sequence)\n",
    "        return inp_lstm_output, state_h\n",
    "\n",
    "    def initialize_hidden_states(self):\n",
    "        return [tf.zeros([self.batch_size, self.inp_lstm_cells]), tf.zeros([self.batch_size, self.inp_lstm_cells])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = Encoder(inp_vocab_size=vocab_size_input, inp_embed_size=inp_embed_size, inp_lstm_cells=inp_lstm_cells,\n",
    "                    batch_size=batch_size, inp_len=length_input)\n",
    "initialized_hidden_states = enc_model.initialize_hidden_states()\n",
    "\n",
    "enc_out, enc_state = enc_model(lang_input[:batch_size], initialized_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(attention_units)\n",
    "        self.W2 = tf.keras.layers.Dense(attention_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden, output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(hidden_with_time_axis) + self.W2(output)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, tar_embed_size, tar_vocab_size, tar_lstm_cells, attention_units,\n",
    "                 batch_size=batch_size, tar_len=length_output):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tar_embed_size = tar_embed_size \n",
    "        self.tar_vocab_size = tar_vocab_size \n",
    "        self.tar_lstm_cells = tar_lstm_cells \n",
    "        self.batch_size = batch_size         \n",
    "        self.tar_len = tar_len              \n",
    "        self.attention_units = attention_units  \n",
    "        self.dec_embedding = Embedding(self.tar_vocab_size, self.tar_embed_size, trainable=True)\n",
    "        self.lstm = LSTM(self.tar_lstm_cells, return_sequences=True, return_state=True)\n",
    "        self.attention = Attention(self.attention_units)\n",
    "        self.final_layer = Dense(self.tar_vocab_size)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        emb_output = self.dec_embedding(x)\n",
    "        x_context = tf.concat([tf.expand_dims(context_vector, axis=1), emb_output], axis=-1)\n",
    "        tar_lstm_output, tar_state_h, tar_state_c = self.lstm(x_context)\n",
    "        tar_lstm_output_reshaped = tf.reshape(tar_lstm_output, shape=(-1, tar_lstm_output.shape[2]))\n",
    "        word_prob = self.final_layer(tar_lstm_output_reshaped)\n",
    "        return word_prob, tar_state_h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_model = Decoder(tar_embed_size=tar_embed_size, tar_vocab_size=vocab_size_output, tar_lstm_cells=tar_lstm_cells,\n",
    "                    attention_units=attention_units)\n",
    "dec_out, dec_state, atn_w = dec_model(tf.random.uniform((batch_size, 1)), enc_state, enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(lang_inp, lang_out):\n",
    "    n_elem = lang_inp.shape[0]\n",
    "    indices = np.random.choice(n_elem, size=n_elem, replace=False)\n",
    "    return lang_inp[indices], lang_out[indices]\n",
    "\n",
    "def generator(batch_number, lang_input, lang_output):\n",
    "    if len(lang_input) <= batch_number*batch_size+batch_size:\n",
    "        return (lang_input[batch_number*batch_size:],\n",
    "            lang_output[batch_number*batch_size:])\n",
    "    return (lang_input[batch_number*batch_size: batch_number*batch_size+batch_size],\n",
    "            lang_output[batch_number*batch_size: batch_number*batch_size+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = enc_model(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([tokenizer_output.word_index['start']] * batch_size, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "          predictions, dec_hidden, _ = dec_model(dec_input, dec_hidden, enc_output)\n",
    "          loss += loss_function(targ[:, t], predictions)\n",
    "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = enc_model.trainable_variables + dec_model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9180\n",
      "Epoch 1 Loss 1.3449\n",
      "Time taken for 1 epoch 86.26547193527222 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1716\n",
      "Epoch 2 Loss 1.2209\n",
      "Time taken for 1 epoch 56.151899099349976 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1553\n",
      "Epoch 3 Loss 1.1791\n",
      "Time taken for 1 epoch 57.35456156730652 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3631\n",
      "Epoch 4 Loss 1.1344\n",
      "Time taken for 1 epoch 57.23469591140747 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.0257\n",
      "Epoch 5 Loss 1.0940\n",
      "Time taken for 1 epoch 57.29439067840576 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9699\n",
      "Epoch 6 Loss 1.0573\n",
      "Time taken for 1 epoch 57.369025230407715 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0326\n",
      "Epoch 7 Loss 1.0218\n",
      "Time taken for 1 epoch 56.324740171432495 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.9220\n",
      "Epoch 8 Loss 0.9906\n",
      "Time taken for 1 epoch 58.20165538787842 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1975\n",
      "Epoch 9 Loss 0.9603\n",
      "Time taken for 1 epoch 57.26634669303894 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.8178\n",
      "Epoch 10 Loss 0.9296\n",
      "Time taken for 1 epoch 56.541563749313354 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.6658\n",
      "Epoch 11 Loss 0.9008\n",
      "Time taken for 1 epoch 55.86750769615173 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.9966\n",
      "Epoch 12 Loss 0.8704\n",
      "Time taken for 1 epoch 56.26391816139221 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.9100\n",
      "Epoch 13 Loss 0.8407\n",
      "Time taken for 1 epoch 56.12339925765991 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6454\n",
      "Epoch 14 Loss 0.8108\n",
      "Time taken for 1 epoch 56.07709264755249 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.7283\n",
      "Epoch 15 Loss 0.7804\n",
      "Time taken for 1 epoch 56.058449268341064 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.6159\n",
      "Epoch 16 Loss 0.7478\n",
      "Time taken for 1 epoch 56.12649893760681 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5800\n",
      "Epoch 17 Loss 0.7163\n",
      "Time taken for 1 epoch 55.957255125045776 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5400\n",
      "Epoch 18 Loss 0.6828\n",
      "Time taken for 1 epoch 56.061214447021484 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.7606\n",
      "Epoch 19 Loss 0.6517\n",
      "Time taken for 1 epoch 55.98914074897766 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5554\n",
      "Epoch 20 Loss 0.6199\n",
      "Time taken for 1 epoch 55.834129095077515 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.6746\n",
      "Epoch 21 Loss 0.5883\n",
      "Time taken for 1 epoch 56.08896231651306 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.4677\n",
      "Epoch 22 Loss 0.5589\n",
      "Time taken for 1 epoch 56.093374729156494 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.4853\n",
      "Epoch 23 Loss 0.5297\n",
      "Time taken for 1 epoch 56.04881811141968 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4614\n",
      "Epoch 24 Loss 0.4984\n",
      "Time taken for 1 epoch 55.94818043708801 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.4167\n",
      "Epoch 25 Loss 0.4687\n",
      "Time taken for 1 epoch 56.192996978759766 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.3949\n",
      "Epoch 26 Loss 0.4413\n",
      "Time taken for 1 epoch 55.94151306152344 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4102\n",
      "Epoch 27 Loss 0.4170\n",
      "Time taken for 1 epoch 55.922192335128784 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.3372\n",
      "Epoch 28 Loss 0.3952\n",
      "Time taken for 1 epoch 55.774513483047485 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.4001\n",
      "Epoch 29 Loss 0.3713\n",
      "Time taken for 1 epoch 56.14400672912598 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.3776\n",
      "Epoch 30 Loss 0.3468\n",
      "Time taken for 1 epoch 56.314860582351685 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.2806\n",
      "Epoch 31 Loss 0.3235\n",
      "Time taken for 1 epoch 56.33606576919556 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2362\n",
      "Epoch 32 Loss 0.3028\n",
      "Time taken for 1 epoch 56.28511309623718 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.2936\n",
      "Epoch 33 Loss 0.2828\n",
      "Time taken for 1 epoch 56.340014696121216 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.2318\n",
      "Epoch 34 Loss 0.2664\n",
      "Time taken for 1 epoch 55.869852781295776 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.2065\n",
      "Epoch 35 Loss 0.2473\n",
      "Time taken for 1 epoch 55.85781669616699 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.2379\n",
      "Epoch 36 Loss 0.2304\n",
      "Time taken for 1 epoch 55.90816330909729 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.1447\n",
      "Epoch 37 Loss 0.2139\n",
      "Time taken for 1 epoch 55.86042809486389 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.2398\n",
      "Epoch 38 Loss 0.1991\n",
      "Time taken for 1 epoch 56.11062502861023 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.1393\n",
      "Epoch 39 Loss 0.1855\n",
      "Time taken for 1 epoch 55.91290593147278 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1583\n",
      "Epoch 40 Loss 0.1725\n",
      "Time taken for 1 epoch 56.17865514755249 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2192\n",
      "Epoch 41 Loss 0.1650\n",
      "Time taken for 1 epoch 55.86897850036621 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1398\n",
      "Epoch 42 Loss 0.1583\n",
      "Time taken for 1 epoch 55.88366961479187 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.1511\n",
      "Epoch 43 Loss 0.1438\n",
      "Time taken for 1 epoch 55.88282537460327 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1372\n",
      "Epoch 44 Loss 0.1269\n",
      "Time taken for 1 epoch 55.876588344573975 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.1360\n",
      "Epoch 45 Loss 0.1137\n",
      "Time taken for 1 epoch 55.82767653465271 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0975\n",
      "Epoch 46 Loss 0.1044\n",
      "Time taken for 1 epoch 55.919597148895264 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0767\n",
      "Epoch 47 Loss 0.0956\n",
      "Time taken for 1 epoch 55.849403619766235 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0648\n",
      "Epoch 48 Loss 0.0880\n",
      "Time taken for 1 epoch 55.91873478889465 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0674\n",
      "Epoch 49 Loss 0.0808\n",
      "Time taken for 1 epoch 55.8733856678009 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0861\n",
      "Epoch 50 Loss 0.0738\n",
      "Time taken for 1 epoch 55.89326810836792 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0646\n",
      "Epoch 51 Loss 0.0711\n",
      "Time taken for 1 epoch 55.94241213798523 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0783\n",
      "Epoch 52 Loss 0.0767\n",
      "Time taken for 1 epoch 55.954277992248535 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0514\n",
      "Epoch 53 Loss 0.0687\n",
      "Time taken for 1 epoch 55.88033580780029 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0547\n",
      "Epoch 54 Loss 0.0618\n",
      "Time taken for 1 epoch 55.85481667518616 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0416\n",
      "Epoch 55 Loss 0.0522\n",
      "Time taken for 1 epoch 56.08188033103943 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0432\n",
      "Epoch 56 Loss 0.0453\n",
      "Time taken for 1 epoch 55.82622146606445 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0335\n",
      "Epoch 57 Loss 0.0417\n",
      "Time taken for 1 epoch 56.00939345359802 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0316\n",
      "Epoch 58 Loss 0.0375\n",
      "Time taken for 1 epoch 55.932328939437866 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0320\n",
      "Epoch 59 Loss 0.0341\n",
      "Time taken for 1 epoch 56.53427577018738 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0317\n",
      "Epoch 60 Loss 0.0305\n",
      "Time taken for 1 epoch 56.27657127380371 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0189\n",
      "Epoch 61 Loss 0.0280\n",
      "Time taken for 1 epoch 55.909921169281006 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0158\n",
      "Epoch 62 Loss 0.0257\n",
      "Time taken for 1 epoch 56.169455766677856 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0179\n",
      "Epoch 63 Loss 0.0238\n",
      "Time taken for 1 epoch 56.09174394607544 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0230\n",
      "Epoch 64 Loss 0.0459\n",
      "Time taken for 1 epoch 56.03428840637207 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0529\n",
      "Epoch 65 Loss 0.0598\n",
      "Time taken for 1 epoch 56.125834703445435 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0420\n",
      "Epoch 66 Loss 0.0435\n",
      "Time taken for 1 epoch 56.017587423324585 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0221\n",
      "Epoch 67 Loss 0.0291\n",
      "Time taken for 1 epoch 55.98691153526306 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0214\n",
      "Epoch 68 Loss 0.0214\n",
      "Time taken for 1 epoch 55.97631764411926 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0118\n",
      "Epoch 69 Loss 0.0179\n",
      "Time taken for 1 epoch 55.9790153503418 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0146\n",
      "Epoch 70 Loss 0.0158\n",
      "Time taken for 1 epoch 56.00999188423157 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0109\n",
      "Epoch 71 Loss 0.0146\n",
      "Time taken for 1 epoch 55.97871446609497 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0110\n",
      "Epoch 72 Loss 0.0136\n",
      "Time taken for 1 epoch 56.20329737663269 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0163\n",
      "Epoch 73 Loss 0.0129\n",
      "Time taken for 1 epoch 55.85633444786072 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0079\n",
      "Epoch 74 Loss 0.0122\n",
      "Time taken for 1 epoch 55.9711549282074 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0058\n",
      "Epoch 75 Loss 0.0117\n",
      "Time taken for 1 epoch 55.97706842422485 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0068\n",
      "Epoch 76 Loss 0.0112\n",
      "Time taken for 1 epoch 55.987853050231934 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0050\n",
      "Epoch 77 Loss 0.0109\n",
      "Time taken for 1 epoch 56.39229106903076 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0117\n",
      "Epoch 78 Loss 0.0108\n",
      "Time taken for 1 epoch 55.97283744812012 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0063\n",
      "Epoch 79 Loss 0.0105\n",
      "Time taken for 1 epoch 55.96005296707153 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0053\n",
      "Epoch 80 Loss 0.0120\n",
      "Time taken for 1 epoch 55.87510275840759 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0077\n",
      "Epoch 81 Loss 0.0285\n",
      "Time taken for 1 epoch 55.973654985427856 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0352\n",
      "Epoch 82 Loss 0.0511\n",
      "Time taken for 1 epoch 55.88947939872742 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0209\n",
      "Epoch 83 Loss 0.0374\n",
      "Time taken for 1 epoch 55.93788242340088 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0162\n",
      "Epoch 84 Loss 0.0216\n",
      "Time taken for 1 epoch 55.877448081970215 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0191\n",
      "Epoch 85 Loss 0.0137\n",
      "Time taken for 1 epoch 55.96366214752197 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0107\n",
      "Epoch 86 Loss 0.0099\n",
      "Time taken for 1 epoch 55.93136644363403 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0036\n",
      "Epoch 87 Loss 0.0087\n",
      "Time taken for 1 epoch 55.88353776931763 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0059\n",
      "Epoch 88 Loss 0.0082\n",
      "Time taken for 1 epoch 55.95580315589905 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0070\n",
      "Epoch 89 Loss 0.0079\n",
      "Time taken for 1 epoch 56.408891439437866 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0067\n",
      "Epoch 90 Loss 0.0076\n",
      "Time taken for 1 epoch 55.94606041908264 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0049\n",
      "Epoch 91 Loss 0.0074\n",
      "Time taken for 1 epoch 55.7774977684021 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0053\n",
      "Epoch 92 Loss 0.0072\n",
      "Time taken for 1 epoch 55.86801767349243 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0081\n",
      "Epoch 93 Loss 0.0070\n",
      "Time taken for 1 epoch 55.788788080215454 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0045\n",
      "Epoch 94 Loss 0.0070\n",
      "Time taken for 1 epoch 55.93435454368591 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0025\n",
      "Epoch 95 Loss 0.0071\n",
      "Time taken for 1 epoch 55.82619595527649 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0071\n",
      "Epoch 96 Loss 0.0070\n",
      "Time taken for 1 epoch 55.930153131484985 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0032\n",
      "Epoch 97 Loss 0.0074\n",
      "Time taken for 1 epoch 55.85257601737976 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0069\n",
      "Epoch 98 Loss 0.0078\n",
      "Time taken for 1 epoch 55.82889246940613 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0090\n",
      "Epoch 99 Loss 0.0132\n",
      "Time taken for 1 epoch 55.7055606842041 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0086\n",
      "Epoch 100 Loss 0.0320\n",
      "Time taken for 1 epoch 55.92207932472229 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0377\n",
      "Epoch 101 Loss 0.0329\n",
      "Time taken for 1 epoch 55.88596963882446 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0177\n",
      "Epoch 102 Loss 0.0190\n",
      "Time taken for 1 epoch 55.96850776672363 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0114\n",
      "Epoch 103 Loss 0.0118\n",
      "Time taken for 1 epoch 55.915730714797974 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0080\n",
      "Epoch 104 Loss 0.0079\n",
      "Time taken for 1 epoch 55.867464780807495 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0130\n",
      "Epoch 105 Loss 0.0068\n",
      "Time taken for 1 epoch 56.01375198364258 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0021\n",
      "Epoch 106 Loss 0.0065\n",
      "Time taken for 1 epoch 56.05467748641968 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0019\n",
      "Epoch 107 Loss 0.0063\n",
      "Time taken for 1 epoch 55.92122960090637 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0047\n",
      "Epoch 108 Loss 0.0061\n",
      "Time taken for 1 epoch 55.975083351135254 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0069\n",
      "Epoch 109 Loss 0.0059\n",
      "Time taken for 1 epoch 55.81715202331543 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0015\n",
      "Epoch 110 Loss 0.0058\n",
      "Time taken for 1 epoch 55.892330169677734 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0026\n",
      "Epoch 111 Loss 0.0058\n",
      "Time taken for 1 epoch 55.96256685256958 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0020\n",
      "Epoch 112 Loss 0.0057\n",
      "Time taken for 1 epoch 55.83822989463806 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0037\n",
      "Epoch 113 Loss 0.0057\n",
      "Time taken for 1 epoch 55.9574921131134 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0009\n",
      "Epoch 114 Loss 0.0056\n",
      "Time taken for 1 epoch 55.81817078590393 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0055\n",
      "Epoch 115 Loss 0.0057\n",
      "Time taken for 1 epoch 55.95296907424927 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0018\n",
      "Epoch 116 Loss 0.0061\n",
      "Time taken for 1 epoch 55.83571481704712 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0113\n",
      "Epoch 117 Loss 0.0059\n",
      "Time taken for 1 epoch 55.86303949356079 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0010\n",
      "Epoch 118 Loss 0.0061\n",
      "Time taken for 1 epoch 55.85379600524902 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0015\n",
      "Epoch 119 Loss 0.0062\n",
      "Time taken for 1 epoch 55.859734535217285 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0081\n",
      "Epoch 120 Loss 0.0081\n",
      "Time taken for 1 epoch 55.859195709228516 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0166\n",
      "Epoch 121 Loss 0.0327\n",
      "Time taken for 1 epoch 55.85664343833923 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0341\n",
      "Epoch 122 Loss 0.0293\n",
      "Time taken for 1 epoch 55.99079370498657 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0091\n",
      "Epoch 123 Loss 0.0147\n",
      "Time taken for 1 epoch 56.02396869659424 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0210\n",
      "Epoch 124 Loss 0.0085\n",
      "Time taken for 1 epoch 55.90076732635498 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0102\n",
      "Epoch 125 Loss 0.0069\n",
      "Time taken for 1 epoch 55.96114683151245 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 125\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "number_of_batches = lang_input.shape[0]//batch_size\n",
    "\n",
    "lang_input_split = lang_input[:number_of_batches*batch_size]\n",
    "lang_output_split = lang_output[:number_of_batches*batch_size]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = enc_model.initialize_hidden_states()\n",
    "    total_loss = 0\n",
    "    lang_inp, lang_out = shuffler(lang_input_split, lang_output_split)\n",
    "    for batch_number in range(number_of_batches):\n",
    "        inp, targ = generator(batch_number, lang_inp, lang_out)\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch_number % 200 == 0:\n",
    "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch_number,\n",
    "                                                       batch_loss.numpy()))\n",
    "    loss_history.append(total_loss / number_of_batches)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / number_of_batches))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.save_weights('tolaki_ina_encoder.weights.h5')\n",
    "dec_model.save_weights('tolaki_ina_decoder.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = 'start ' + w + ' end'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocessing_sentence(sentence)\n",
    "    inputs = [tokenizer_input.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                             maxlen=length_input,\n",
    "                                                             padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros([1, inp_lstm_cells]), tf.zeros([1, inp_lstm_cells])]\n",
    "    enc_out, enc_hidden = enc_model(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer_output.word_index['start']], 0)\n",
    "\n",
    "    for t in range(length_output):\n",
    "        predictions, dec_hidden, attention_weights = dec_model(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        if tokenizer_output.index_word[predicted_id] == 'end':\n",
    "            return result, sentence\n",
    "\n",
    "        result += tokenizer_output.index_word[predicted_id] + ' '\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    inp_vocab_size = len(tokenizer_input.word_index) + 1\n",
    "    tar_vocab_size = len(tokenizer_output.word_index) + 1\n",
    "\n",
    "    # Buat instance model\n",
    "    enc_model = Encoder(inp_vocab_size=inp_vocab_size, inp_embed_size=inp_embed_size, \n",
    "                        inp_lstm_cells=inp_lstm_cells, batch_size=batch_size, inp_len=length_input)\n",
    "    dec_model = Decoder(tar_embed_size=tar_embed_size, tar_vocab_size=tar_vocab_size, \n",
    "                        tar_lstm_cells=tar_lstm_cells, attention_units=attention_units, \n",
    "                        batch_size=batch_size, tar_len=length_output)\n",
    "\n",
    "    # Panggil model untuk inisialisasi variabel\n",
    "    dummy_input_enc = tf.zeros((batch_size, length_input), dtype=tf.int32)\n",
    "    hidden_states_enc = enc_model.initialize_hidden_states()\n",
    "    enc_model(dummy_input_enc, hidden_states_enc)\n",
    "\n",
    "    dummy_input_dec = tf.zeros((batch_size, 1), dtype=tf.int32)\n",
    "    hidden_state_enc = hidden_states_enc[0]\n",
    "    dummy_enc_output = tf.zeros((batch_size, length_input, inp_lstm_cells))\n",
    "    dec_model(dummy_input_dec, hidden_state_enc, dummy_enc_output)\n",
    "\n",
    "    # Muat bobot\n",
    "    try:\n",
    "        enc_model.load_weights('tolaki_ina_encoder.weights.h5')\n",
    "        dec_model.load_weights('tolaki_ina_decoder.weights.h5')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Verifikasi struktur model\n",
    "    print(\"Encoder model summary:\")\n",
    "    enc_model.summary()\n",
    "    print(\"Decoder model summary:\")\n",
    "    dec_model.summary()\n",
    "\n",
    "    return enc_model, dec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat tokenizer dari JSON\n",
    "with open('tolaki_ina_tokenizer_input.json', 'r') as f:\n",
    "    tokenizer_input_json = json.load(f)\n",
    "    tokenizer_input = tokenizer_from_json(tokenizer_input_json)\n",
    "\n",
    "with open('tolaki_ina_tokenizer_output.json', 'r') as f:\n",
    "    tokenizer_output_json = json.load(f)\n",
    "    tokenizer_output = tokenizer_from_json(tokenizer_output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">558,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>))       │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m558,336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m,   │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m))       │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">952,576</span> (3.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m952,576\u001b[0m (3.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">952,576</span> (3.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m952,576\u001b[0m (3.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">376,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,841</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2943</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">756,351</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m376,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m,    │       \u001b[38;5;34m656,384\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_1 (\u001b[38;5;33mAttention\u001b[0m)         │ ?                      │       \u001b[38;5;34m131,841\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m2943\u001b[0m)             │       \u001b[38;5;34m756,351\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,921,280</span> (7.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,921,280\u001b[0m (7.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,921,280</span> (7.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,921,280\u001b[0m (7.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Memuat model\n",
    "enc_model, dec_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat Bahasa Tolaki: start aria a mbotingu end\n",
      "Hasil Terjemahan: dari dalam keranjang \n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan untuk menerjemahkan kalimat\n",
    "translated_sentence, original_sentence = evaluate(\"aria a mbotingu\")\n",
    "print(f\"Kalimat Bahasa Tolaki: {original_sentence}\")\n",
    "print(f\"Hasil Terjemahan: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
